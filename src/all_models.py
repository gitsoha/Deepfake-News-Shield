# -*- coding: utf-8 -*-
"""Untitled17.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z9d-Q6oOIfnpPBzgil90YmKW7A3O6irX
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

cleaned_path = "/content/drive/MyDrive/Major_Project/IFND_dataset/final_IFND_clean.csv"
df = pd.read_csv(cleaned_path)
print("Shape:", df.shape)
df.head()

# Convert labels to uppercase (safety)
df["Label"] = df["Label"].astype(str).str.upper().str.strip()

# Count REAL and FAKE
label_counts = df["Label"].value_counts()

print(label_counts)

from sklearn.model_selection import train_test_split

# Use only the required columns
data = df[['id','clean_text','Label']].copy()

# 80% train, 20% temporary (for val+test)
train_df, temp_df = train_test_split(
    data, test_size=0.2, random_state=42, stratify=data['Label']
)

# Split the 20% temporary into 10% val and 10% test
val_df, test_df = train_test_split(
    temp_df, test_size=0.5, random_state=42, stratify=temp_df['Label']
)

print("Train:", train_df.shape, "Val:", val_df.shape, "Test:", test_df.shape)
train_df.to_csv("/content/drive/MyDrive/Major_Project/IFND_dataset/IFND_train.csv", index=False)
val_df.to_csv("/content/drive/MyDrive/Major_Project/IFND_dataset/IFND_val.csv", index=False)
test_df.to_csv("/content/drive/MyDrive/Major_Project/IFND_dataset/IFND_test.csv", index=False)

print("‚úÖ Saved splits successfully!")

import pandas as pd
base="/content/drive/MyDrive/Major_Project/IFND_dataset"
for f in ["IFND_train.csv","IFND_val.csv","IFND_test.csv"]:
    d=pd.read_csv(f"{base}/{f}")
    print(f, d['Label'].value_counts(normalize=True).round(3))

# 5.1 ‚Äî Imports, config, paths
import os, random, numpy as np, pandas as pd, torch
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import classification_report, f1_score, confusion_matrix
from sklearn.utils.class_weight import compute_class_weight
from transformers import AutoTokenizer, AutoModelForSequenceClassification, get_linear_schedule_with_warmup
from torch.optim import AdamW
from tqdm import tqdm
import torch.nn.functional as F

# Repro
SEED = 42
random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

# Paths
BASE = "/content/drive/MyDrive/Major_Project/IFND_dataset"
TRAIN_PATH = f"{BASE}/IFND_train.csv"
VAL_PATH   = f"{BASE}/IFND_val.csv"
TEST_PATH  = f"{BASE}/IFND_test.csv"

# Labels
LABEL2ID = {"FALSE":0, "TRUE":1}
ID2LABEL = {v:k for k,v in LABEL2ID.items()}

# Hyperparams
MODEL_NAME = "roberta-base"
MAX_LEN = 128
BATCH_SIZE = 16
EPOCHS = 15              # updated
LR = 2e-5
WARMUP_RATIO = 0.1
PATIENCE = 2             # updated

SAVE_DIR = f"{BASE}/models_roberta"
os.makedirs(SAVE_DIR, exist_ok=True)

# 5.2 ‚Äî Load splits + encode labels + class weights
train_df = pd.read_csv(TRAIN_PATH)
val_df   = pd.read_csv(VAL_PATH)
test_df  = pd.read_csv(TEST_PATH)

for d in [train_df, val_df, test_df]:
    d["Label"] = d["Label"].astype(str).str.upper().map(LABEL2ID)

# class weights for imbalance (~68/32)
classes = np.array([0,1])
cw = compute_class_weight(class_weight="balanced", classes=classes, y=train_df["Label"].values)
class_weights = torch.tensor(cw, dtype=torch.float32).to(device)
print("Class weights [FALSE, TRUE]:", cw)
print(train_df.shape, val_df.shape, test_df.shape)

# 5.3 ‚Äî Dataset class
class TextClsDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_len):
        self.texts = texts.tolist()
        self.labels = labels.tolist()
        self.tok = tokenizer
        self.max_len = max_len
    def __len__(self): return len(self.texts)
    def __getitem__(self, idx):
        enc = self.tok(
            str(self.texts[idx]),
            max_length=self.max_len,
            truncation=True,
            padding="max_length",
            return_tensors="pt"
        )
        item = {k: v.squeeze(0) for k,v in enc.items()}
        item["labels"] = torch.tensor(int(self.labels[idx]), dtype=torch.long)
        return item

# 5.4 ‚Äî Tokenizer & Dataloaders
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

train_ds = TextClsDataset(train_df["clean_text"], train_df["Label"], tokenizer, MAX_LEN)
val_ds   = TextClsDataset(val_df["clean_text"],   val_df["Label"],   tokenizer, MAX_LEN)
test_ds  = TextClsDataset(test_df["clean_text"],  test_df["Label"],  tokenizer, MAX_LEN)

train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  pin_memory=True)
val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)
test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)

len(train_loader), len(val_loader), len(test_loader)

# 5.5 ‚Äî Model, optimizer, scheduler, AMP scaler
model = AutoModelForSequenceClassification.from_pretrained(
    MODEL_NAME, num_labels=2, id2label=ID2LABEL, label2id=LABEL2ID
).to(device)

optimizer = AdamW(model.parameters(), lr=LR)

total_steps = len(train_loader) * EPOCHS
warmup_steps = int(total_steps * WARMUP_RATIO)
scheduler = get_linear_schedule_with_warmup(
    optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps
)

scaler = torch.cuda.amp.GradScaler(enabled=(device.type=="cuda"))
print("Total steps:", total_steps, "Warmup:", warmup_steps)

# 5.6 ‚Äî Eval helper (loss + macro-F1)
@torch.no_grad()
def run_eval(model, data_loader, class_weights=None):
    model.eval()
    all_preds, all_labels = [], []
    eval_loss = 0.0
    for batch in data_loader:
        batch = {k: v.to(device) for k,v in batch.items()}
        labels = batch.pop("labels")
        outputs = model(**batch)
        logits = outputs.logits
        loss = F.cross_entropy(logits, labels, weight=class_weights)
        eval_loss += loss.item()
        preds = torch.argmax(logits, dim=1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())
    avg_loss = eval_loss / max(1, len(data_loader))
    f1 = f1_score(all_labels, all_preds, average="macro")
    return avg_loss, f1, np.array(all_labels), np.array(all_preds)

# 5.7 ‚Äî Train loop (AMP + early stopping on macro-F1)
best_f1 = -1
wait = 0
best_path = os.path.join(SAVE_DIR, "roberta-best.pt")

for epoch in range(1, EPOCHS+1):
    model.train()
    running = 0.0
    pbar = tqdm(train_loader, desc=f"Epoch {epoch}/{EPOCHS}")
    for batch in pbar:
        batch = {k: v.to(device) for k,v in batch.items()}
        labels = batch.pop("labels")

        optimizer.zero_grad(set_to_none=True)
        with torch.cuda.amp.autocast(enabled=(device.type=="cuda")):
            outputs = model(**batch)
            logits = outputs.logits
            loss = F.cross_entropy(logits, labels, weight=class_weights)

        scaler.scale(loss).backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
        scaler.step(optimizer)
        scaler.update()
        scheduler.step()

        running += loss.item()
        pbar.set_postfix({"train_loss": f"{running/(pbar.n or 1):.4f}"})

    val_loss, val_f1, y_true, y_pred = run_eval(model, val_loader, class_weights)
    print(f"\nEpoch {epoch}: val_loss={val_loss:.4f} | val_macroF1={val_f1:.4f}")

    if val_f1 > best_f1:
        best_f1 = val_f1; wait = 0
        torch.save(model.state_dict(), best_path)
        print(f"‚úÖ New best saved ‚Üí {best_path}")
    else:
        wait += 1
        if wait >= PATIENCE:
            print("‚èπÔ∏è Early stopping (no val F1 improvement).")
            break

print("Best Val macro-F1:", best_f1)

# 5.8 ‚Äî Load best + test metrics
model.load_state_dict(torch.load(best_path, map_location=device))
model.to(device)

test_loss, test_f1, y_true, y_pred = run_eval(model, test_loader, class_weights)
print(f"TEST ‚Äî loss: {test_loss:.4f} | macro-F1: {test_f1:.4f}")
print("\nClassification Report:\n", classification_report(y_true, y_pred, target_names=["FALSE","TRUE"]))
print("Confusion Matrix:\n", confusion_matrix(y_true, y_pred))

# 5.9 ‚Äî Accuracy, probabilities, ROC-AUC (Test)
from sklearn.metrics import accuracy_score, roc_auc_score
import pandas as pd

model.eval()
y_probs, y_true_list = [], []

with torch.no_grad():
    for batch in test_loader:
        labels_np = batch["labels"].numpy()
        y_true_list.extend(labels_np)

        batch = {k: v.to(device) for k,v in batch.items()}
        labels_t = batch.pop("labels")
        logits = model(**batch).logits
        probs = F.softmax(logits, dim=1)[:, 1]  # P(TRUE)
        y_probs.extend(probs.detach().cpu().numpy())

y_true_arr = np.array(y_true_list)
y_prob_arr = np.array(y_probs)
y_pred_arr = (y_prob_arr >= 0.5).astype(int)

test_acc = accuracy_score(y_true_arr, y_pred_arr)
test_auc = roc_auc_score(y_true_arr, y_prob_arr)

print(f"TEST Accuracy: {test_acc:.4f}")
print(f"TEST ROC-AUC : {test_auc:.4f}")

preds_df = pd.DataFrame({"y_true": y_true_arr, "y_prob_TRUE": y_prob_arr, "y_pred": y_pred_arr})
preds_path = os.path.join(SAVE_DIR, "roberta_test_predictions.csv")
preds_df.to_csv(preds_path, index=False)
print("Saved per-sample predictions ‚Üí", preds_path)

# 5.10 ‚Äî ROC Curve (plot + save)
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

fpr, tpr, thresholds = roc_curve(y_true_arr, y_prob_arr)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(6,5))
plt.plot(fpr, tpr, label=f"RoBERTa ROC (AUC = {roc_auc:.3f})")
plt.plot([0,1], [0,1], linestyle="--")
plt.xlim([0.0, 1.0]); plt.ylim([0.0, 1.05])
plt.xlabel("False Positive Rate"); plt.ylabel("True Positive Rate")
plt.title("ROC Curve ‚Äî RoBERTa (Test)")
plt.legend(loc="lower right"); plt.grid(True)

roc_path = os.path.join(SAVE_DIR, "roberta_roc_curve.png")
plt.savefig(roc_path, bbox_inches="tight", dpi=150)
plt.show()
print("Saved ROC curve image ‚Üí", roc_path)

# ============================================
# STEP 5.11 ‚Äî Train vs Test ROC Curve & AUC
# ============================================
import torch
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
from tqdm import tqdm

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(DEVICE)
model.eval()

def get_roc_data(dataloader, name="Test"):
    y_true, y_scores = [], []
    with torch.no_grad():
        for batch in tqdm(dataloader, desc=f"Generating ROC data ({name})"):
            input_ids = batch["input_ids"].to(DEVICE)
            attention_mask = batch["attention_mask"].to(DEVICE)
            labels = batch["labels"].cpu().numpy()
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            probs = torch.softmax(outputs.logits, dim=1)[:, 1].cpu().numpy()
            y_true.extend(labels)
            y_scores.extend(probs)
    fpr, tpr, _ = roc_curve(y_true, y_scores)
    auc_score = auc(fpr, tpr)
    return fpr, tpr, auc_score

# ‚úÖ Generate ROC data for both train & test
fpr_train, tpr_train, auc_train = get_roc_data(train_loader, name="Train")
fpr_test, tpr_test, auc_test = get_roc_data(test_loader, name="Test")

# ‚úÖ Plot both ROC curves
plt.figure(figsize=(7, 6))
plt.plot(fpr_train, tpr_train, label=f"Train (AUC = {auc_train:.4f})", color="#2ca02c", lw=2.5)
plt.plot(fpr_test, tpr_test, label=f"Test (AUC = {auc_test:.4f})", color="#1f77b4", lw=2.5)
plt.plot([0, 1], [0, 1], linestyle="--", color="gray", lw=1.2)

plt.title("Train vs Test ROC Curve ‚Äî RoBERTa", fontsize=13, fontweight="bold", pad=15)
plt.xlabel("False Positive Rate", fontsize=11)
plt.ylabel("True Positive Rate", fontsize=11)
plt.legend(loc="lower right", frameon=True, shadow=True)
plt.grid(alpha=0.25)
plt.tight_layout()
plt.show()

# ‚úÖ Print summary
print(f"‚úÖ Train AUC: {auc_train:.4f}")
print(f"‚úÖ Test  AUC: {auc_test:.4f}")
if abs(auc_train - auc_test) <= 0.02:
    print("üéØ Model generalizes well (no overfitting).")
elif abs(auc_train - auc_test) <= 0.05:
    print("‚ö†Ô∏è Slight overfitting, but acceptable.")
else:
    print("üö® Significant overfitting detected ‚Äî check regularization or data balance.")

from google.colab import drive
drive.mount('/content/drive')

# STEP 6.1 Imports & Config (Bi-LSTM)
import os, random, numpy as np, pandas as pd, torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import classification_report, f1_score, confusion_matrix, accuracy_score, roc_auc_score
from sklearn.utils.class_weight import compute_class_weight
from tqdm import tqdm
import torch.nn.functional as F

# Reproducibility
SEED = 42
random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

# Paths
BASE = "/content/drive/MyDrive/Major_Project/IFND_dataset"
TRAIN_PATH = f"{BASE}/IFND_train.csv"
VAL_PATH   = f"{BASE}/IFND_val.csv"
TEST_PATH  = f"{BASE}/IFND_test.csv"

# Label mapping
LABEL2ID = {"FALSE":0, "TRUE":1}
ID2LABEL = {v:k for k,v in LABEL2ID.items()}

# Save directory
SAVE_DIR = f"{BASE}/models_bilstm"
os.makedirs(SAVE_DIR, exist_ok=True)

# STEP 6.2 Prepare Data + Tokenization (Bi-LSTM)
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

train_df = pd.read_csv(TRAIN_PATH)
val_df   = pd.read_csv(VAL_PATH)
test_df  = pd.read_csv(TEST_PATH)

for d in [train_df, val_df, test_df]:
    d["Label"] = d["Label"].astype(str).str.upper().map(LABEL2ID)

MAX_LEN = 128
tokenizer = Tokenizer(oov_token="<OOV>")
tokenizer.fit_on_texts(train_df["clean_text"])
vocab_size = len(tokenizer.word_index) + 1

def encode_texts(texts):
    seq = tokenizer.texts_to_sequences(texts)
    return pad_sequences(seq, maxlen=MAX_LEN, padding='post', truncating='post')

X_train = encode_texts(train_df["clean_text"])
X_val   = encode_texts(val_df["clean_text"])
X_test  = encode_texts(test_df["clean_text"])

y_train = train_df["Label"].values
y_val   = val_df["Label"].values
y_test  = test_df["Label"].values

print("Vocab size:", vocab_size)

# STEP 6.3 Dataset Class (Bi-LSTM)
class TextDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.long)
        self.y = torch.tensor(y, dtype=torch.long)
    def __len__(self): return len(self.X)
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

train_ds = TextDataset(X_train, y_train)
val_ds   = TextDataset(X_val, y_val)
test_ds  = TextDataset(X_test, y_test)

BATCH_SIZE = 64
train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)
val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE)
test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE)

# STEP 6.4 Bi-LSTM Model (Bi-LSTM)
class BiLSTMClassifier(nn.Module):
    def __init__(self, vocab_size, embed_dim=256, hidden_dim=128, num_layers=2, dropout=0.4):
        super().__init__()
        self.hidden_dim = hidden_dim  # ‚úÖ store as class attribute
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers,
                            dropout=dropout, batch_first=True, bidirectional=True)
        self.fc = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(hidden_dim * 2, 128),
            nn.BatchNorm1d(128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, 2)
        )

    def forward(self, x):
        embedded = self.embedding(x)                # [batch, seq_len, embed_dim]
        lstm_out, _ = self.lstm(embedded)           # [batch, seq_len, hidden_dim*2]

        # ‚úÖ Access using self.hidden_dim
        h_forward = lstm_out[:, -1, :self.hidden_dim]
        h_backward = lstm_out[:, 0, self.hidden_dim:]
        x = torch.cat((h_forward, h_backward), dim=1)
        logits = self.fc(x)
        return logits

model = BiLSTMClassifier(vocab_size).to(device)
print(model)

# STEP 6.5 Training Setup (Bi-LSTM)
EPOCHS = 10
LR = 1e-3
PATIENCE = 2

class_weights = compute_class_weight(class_weight="balanced", classes=np.array([0,1]), y=y_train)
class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)
print("Class weights:", class_weights)

optimizer = torch.optim.Adam(model.parameters(), lr=LR)
criterion = nn.CrossEntropyLoss(weight=class_weights)

# ‚úÖ Fix: remove 'verbose' argument for compatibility
from torch.optim.lr_scheduler import ReduceLROnPlateau
scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=1)

# STEP 6.6 Evaluation Helper (Bi-LSTM)
@torch.no_grad()
def evaluate(model, loader):
    model.eval()
    all_preds, all_labels, losses = [], [], []
    for X, y in loader:
        X, y = X.to(device), y.to(device)
        logits = model(X)
        loss = criterion(logits, y)
        preds = torch.argmax(logits, dim=1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(y.cpu().numpy())
        losses.append(loss.item())
    avg_loss = np.mean(losses)
    f1 = f1_score(all_labels, all_preds, average='macro')
    return avg_loss, f1, np.array(all_labels), np.array(all_preds)

# STEP 6.7 Training Loop (Bi-LSTM)
best_f1 = -1
wait = 0
best_path = os.path.join(SAVE_DIR, "bilstm-best.pt")

for epoch in range(1, EPOCHS+1):
    model.train()
    running = 0.0
    for X, y in tqdm(train_loader, desc=f"Epoch {epoch}/{EPOCHS}"):
        X, y = X.to(device), y.to(device)
        optimizer.zero_grad()
        logits = model(X)
        loss = criterion(logits, y)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # ‚úÖ Gradient clipping
        optimizer.step()
        running += loss.item()

    val_loss, val_f1, _, _ = evaluate(model, val_loader)
    scheduler.step(val_f1)

    print(f"\nEpoch {epoch}: train_loss={running/len(train_loader):.4f} | val_loss={val_loss:.4f} | val_F1={val_f1:.4f}")

    if val_f1 > best_f1:
        best_f1 = val_f1; wait = 0
        torch.save({
            'model_state_dict': model.state_dict(),
            'vocab_size': vocab_size,
            'embed_dim': 256,
            'hidden_dim': 128
        }, best_path)
        print(f"‚úÖ New best model saved ‚Üí {best_path}")
    else:
        wait += 1
        if wait >= PATIENCE:
            print("‚èπÔ∏è Early stopping triggered.")
            break

print("Best Val F1:", best_f1)

# STEP 6.8 Load Best + Test Metrics (Bi-LSTM)
checkpoint = torch.load(best_path, map_location=device)
model = BiLSTMClassifier(checkpoint['vocab_size'],
                         embed_dim=checkpoint['embed_dim'],
                         hidden_dim=checkpoint['hidden_dim']).to(device)
model.load_state_dict(checkpoint['model_state_dict'])

test_loss, test_f1, y_true, y_pred = evaluate(model, test_loader)
print(f"TEST ‚Äî loss: {test_loss:.4f} | macro-F1: {test_f1:.4f}")
print("\nClassification Report:\n", classification_report(y_true, y_pred, target_names=["FALSE","TRUE"]))
print("Confusion Matrix:\n", confusion_matrix(y_true, y_pred))

# STEP 6.9 ROC-AUC + Accuracy (Bi-LSTM)
from sklearn.metrics import roc_curve, auc

model.eval()
y_true_list, y_probs = [], []
with torch.no_grad():
    for X, y in test_loader:
        X = X.to(device)
        logits = model(X)
        probs = F.softmax(logits, dim=1)[:, 1].cpu().numpy()
        y_probs.extend(probs)
        y_true_list.extend(y.numpy())

y_true_arr = np.array(y_true_list)
y_prob_arr = np.array(y_probs)
y_pred_arr = (y_prob_arr >= 0.5).astype(int)

test_acc = accuracy_score(y_true_arr, y_pred_arr)
test_auc = roc_auc_score(y_true_arr, y_prob_arr)
print(f"TEST Accuracy: {test_acc:.4f}")
print(f"TEST ROC-AUC : {test_auc:.4f}")

# STEP 6.10 ROC Curve Plot (Bi-LSTM)
import matplotlib.pyplot as plt

fpr, tpr, _ = roc_curve(y_true_arr, y_prob_arr)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(6,5))
plt.plot(fpr, tpr, label=f"BiLSTM ROC (AUC = {roc_auc:.3f})")
plt.plot([0,1], [0,1], linestyle="--", color="gray")
plt.xlabel("False Positive Rate"); plt.ylabel("True Positive Rate")
plt.title("ROC Curve ‚Äî BiLSTM (Test)")
plt.legend(loc="lower right"); plt.grid(True)
roc_path = os.path.join(SAVE_DIR, "bilstm_roc_curve.png")
plt.savefig(roc_path, bbox_inches="tight", dpi=150)
plt.show()
print("Saved ROC curve image ‚Üí", roc_path)

# ============================================
# STEP 6.11 ‚Äî Train vs Test ROC Curve & AUC (Bi-LSTM)
# ============================================
import torch
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc
from tqdm import tqdm

DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(DEVICE)
model.eval()

def get_roc_data(dataloader, name="Test"):
    y_true, y_scores = [], []
    with torch.no_grad():
        for X, y in tqdm(dataloader, desc=f"Generating ROC data ({name})"):
            X = X.to(DEVICE)
            logits = model(X)
            probs = torch.softmax(logits, dim=1)[:, 1].cpu().numpy()  # P(TRUE)
            y_true.extend(y.numpy())
            y_scores.extend(probs)
    fpr, tpr, _ = roc_curve(y_true, y_scores)
    auc_score = auc(fpr, tpr)
    return fpr, tpr, auc_score

# ‚úÖ Generate ROC data for both train & test
fpr_train, tpr_train, auc_train = get_roc_data(train_loader, name="Train")
fpr_test, tpr_test, auc_test = get_roc_data(test_loader, name="Test")

# ‚úÖ Plot both ROC curves
plt.figure(figsize=(7, 6))
plt.plot(fpr_train, tpr_train, label=f"Train (AUC = {auc_train:.4f})", color="#2ca02c", lw=2.5)
plt.plot(fpr_test, tpr_test, label=f"Test (AUC = {auc_test:.4f})", color="#1f77b4", lw=2.5)
plt.plot([0, 1], [0, 1], linestyle="--", color="gray", lw=1.2)

plt.title("Train vs Test ROC Curve ‚Äî Bi-LSTM", fontsize=13, fontweight="bold", pad=15)
plt.xlabel("False Positive Rate", fontsize=11)
plt.ylabel("True Positive Rate", fontsize=11)
plt.legend(loc="lower right", frameon=True, shadow=True)
plt.grid(alpha=0.25)
plt.tight_layout()
plt.show()

# ‚úÖ Print summary
print(f"‚úÖ Train AUC: {auc_train:.4f}")
print(f"‚úÖ Test  AUC: {auc_test:.4f}")

if abs(auc_train - auc_test) <= 0.02:
    print("üéØ Model generalizes well (no overfitting).")
elif abs(auc_train - auc_test) <= 0.05:
    print("‚ö†Ô∏è Slight overfitting, but acceptable.")
else:
    print("üö® Significant overfitting detected ‚Äî check regularization or data balance.")

# ============================================
# STEP 6.12 ‚Äî Save Test Predictions (Bi-LSTM)
# ============================================

import pandas as pd

preds_df = pd.DataFrame({
    "y_true": y_true_arr,
    "y_prob_TRUE": y_prob_arr,
    "y_pred": y_pred_arr
})

preds_path = os.path.join(SAVE_DIR, "bilstm_test_predictions.csv")
preds_df.to_csv(preds_path, index=False)

print("‚úÖ Saved Bi-LSTM test predictions ‚Üí", preds_path)

from google.colab import drive
drive.mount('/content/drive')

# STEP 7.1 ‚Äî Imports, config, paths (Bi-LSTM + Attn + Stacked GRU)
import os, random, numpy as np, pandas as pd, torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import classification_report, f1_score, confusion_matrix, accuracy_score, roc_auc_score
from sklearn.utils.class_weight import compute_class_weight
from tqdm import tqdm
import matplotlib.pyplot as plt

# Reproducibility
SEED = 42
random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

# Paths
BASE = "/content/drive/MyDrive/Major_Project/IFND_dataset"
TRAIN_PATH = f"{BASE}/IFND_train.csv"
VAL_PATH   = f"{BASE}/IFND_val.csv"
TEST_PATH  = f"{BASE}/IFND_test.csv"

# Label mapping (ensure labels match)
LABEL2ID = {"FALSE":0, "TRUE":1}
ID2LABEL = {v:k for k,v in LABEL2ID.items()}

# Save directory
SAVE_DIR = f"{BASE}/models_hybrid"
os.makedirs(SAVE_DIR, exist_ok=True)

# STEP 7.2 Prepare Data + Tokenization
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

train_df = pd.read_csv(TRAIN_PATH)
val_df   = pd.read_csv(VAL_PATH)
test_df  = pd.read_csv(TEST_PATH)

# map labels (ensure uppercase)
for d in [train_df, val_df, test_df]:
    d["Label"] = d["Label"].astype(str).str.upper().map(LABEL2ID)

MAX_LEN = 128
tokenizer = Tokenizer(oov_token="<OOV>")
tokenizer.fit_on_texts(train_df["clean_text"])
vocab_size = len(tokenizer.word_index) + 1

def encode_texts(texts):
    seq = tokenizer.texts_to_sequences(texts)
    return pad_sequences(seq, maxlen=MAX_LEN, padding='post', truncating='post')

X_train = encode_texts(train_df["clean_text"])
X_val   = encode_texts(val_df["clean_text"])
X_test  = encode_texts(test_df["clean_text"])

y_train = train_df["Label"].values
y_val   = val_df["Label"].values
y_test  = test_df["Label"].values

print("Vocab size:", vocab_size)

# 7.3 Dataset class and DataLoader
class TextDataset(Dataset):
    def __init__(self, X, y):
        self.X = torch.tensor(X, dtype=torch.long)
        self.y = torch.tensor(y, dtype=torch.long)
    def __len__(self): return len(self.X)
    def __getitem__(self, idx):
        return self.X[idx], self.y[idx]

train_ds = TextDataset(X_train, y_train)
val_ds   = TextDataset(X_val, y_val)
test_ds  = TextDataset(X_test, y_test)

BATCH_SIZE = 64
train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)
val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE)
test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE)

# 7.4 HYBRID MODEL: Bi-LSTM + Attention + Stacked GRU

import torch
import torch.nn as nn
import torch.nn.functional as F

class AttentionLayer(nn.Module):
    def __init__(self, hidden_dim):
        super().__init__()
        # input dim = 2*hidden (bidirectional)
        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)
        self.v = nn.Linear(hidden_dim, 1, bias=False)

    def forward(self, lstm_outputs):
        # lstm_outputs: [batch, seq_len, 2*hidden]
        scores = self.v(torch.tanh(self.attn(lstm_outputs))).squeeze(-1)  # [batch, seq_len]
        weights = F.softmax(scores, dim=1)  # [batch, seq_len]
        weighted_output = (lstm_outputs * weights.unsqueeze(-1)).sum(dim=1)  # [batch, 2*hidden]
        return weighted_output, weights

class HybridBiLSTM_Attn_GRU(nn.Module):
    def __init__(self, vocab_size, embed_dim=256, hidden_dim=128,
                 num_gru_layers=1, dropout=0.3):
        super().__init__()

        self.hidden_dim = hidden_dim
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)

        # Bi-LSTM
        self.bilstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True,
                              bidirectional=True, dropout=dropout)

        # Attention
        self.attn = AttentionLayer(hidden_dim)

        # Stacked GRU (takes attn vector as seq_len=1 input)
        self.gru = nn.GRU(hidden_dim * 2, hidden_dim,
                          num_layers=num_gru_layers,
                          batch_first=True,
                          bidirectional=False,
                          dropout=dropout)

        # Classifier
        self.fc = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, 2)
        )

    def forward(self, x):
        # x: [batch, seq_len]
        x = self.embedding(x)  # [batch, seq_len, embed_dim]

        lstm_out, _ = self.bilstm(x)  # [batch, seq_len, 2*hidden]
        attn_vec, attn_weights = self.attn(lstm_out)  # attn_vec: [batch, 2*hidden]

        # make seq for GRU: [batch, 1, 2*hidden]
        gru_input = attn_vec.unsqueeze(1)
        gru_out, _ = self.gru(gru_input)  # [batch, 1, hidden]
        gru_final = gru_out[:, -1, :]     # [batch, hidden]

        logits = self.fc(gru_final)       # [batch, 2]
        return logits

# instantiate model
model = HybridBiLSTM_Attn_GRU(vocab_size, embed_dim=256, hidden_dim=128,
                             num_gru_layers=1, dropout=0.3).to(device)
print(model)

# 7.5 Training setup
EPOCHS = 15
LR = 1e-3
PATIENCE = 3

# compute class weights
class_weights = compute_class_weight(class_weight="balanced", classes=np.array([0,1]), y=y_train)
class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)
print("Class weights:", class_weights)

optimizer = torch.optim.Adam(model.parameters(), lr=LR)
criterion = nn.CrossEntropyLoss(weight=class_weights)

from torch.optim.lr_scheduler import ReduceLROnPlateau
scheduler = ReduceLROnPlateau(
    optimizer,
    mode='max',
    factor=0.5,
    patience=1     # ‚Üê verbose removed here
)

best_f1 = -1
wait = 0
best_path = os.path.join(SAVE_DIR, "hybrid-best.pt")

# 7.6 Evaluation helper
@torch.no_grad()
def evaluate(model, loader):
    model.eval()
    all_preds, all_labels, losses = [], [], []
    for X, y in loader:
        X, y = X.to(device), y.to(device)
        logits = model(X)
        loss = criterion(logits, y)
        preds = torch.argmax(logits, dim=1)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(y.cpu().numpy())
        losses.append(loss.item())
    avg_loss = np.mean(losses) if len(losses)>0 else 0.0
    f1 = f1_score(all_labels, all_preds, average='macro')
    return avg_loss, f1, np.array(all_labels), np.array(all_preds)

# 7.7 Training loop
for epoch in range(1, EPOCHS+1):
    model.train()
    running = 0.0
    for X, y in tqdm(train_loader, desc=f"Epoch {epoch}/{EPOCHS}"):
        X, y = X.to(device), y.to(device)
        optimizer.zero_grad()
        logits = model(X)
        loss = criterion(logits, y)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        running += loss.item()

    train_loss = running / len(train_loader)
    val_loss, val_f1, _, _ = evaluate(model, val_loader)
    scheduler.step(val_f1)

    print(f"\nEpoch {epoch}: train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | val_F1={val_f1:.4f}")

    if val_f1 > best_f1:
        best_f1 = val_f1; wait = 0
        torch.save({
            'model_state_dict': model.state_dict(),
            'vocab_size': vocab_size,
            'embed_dim': 256,
            'hidden_dim': 128,
            'num_gru_layers': 1
        }, best_path)
        print(f"‚úÖ New best model saved ‚Üí {best_path}")
    else:
        wait += 1
        if wait >= PATIENCE:
            print("‚èπÔ∏è Early stopping triggered.")
            break

print("Best Val F1:", best_f1)

# 7.8 Load best and evaluate on test
checkpoint = torch.load(best_path, map_location=device)

model = HybridBiLSTM_Attn_GRU(
    checkpoint['vocab_size'],
    embed_dim=checkpoint['embed_dim'],
    hidden_dim=checkpoint['hidden_dim'],
    num_gru_layers=1,   # üî• SAME as training
    dropout=0.3          # üî• SAME as training
).to(device)

model.load_state_dict(checkpoint['model_state_dict'])

test_loss, test_f1, y_true, y_pred = evaluate(model, test_loader)
print(f"TEST ‚Äî loss: {test_loss:.4f} | macro-F1: {test_f1:.4f}")
print("\nClassification Report:\n", classification_report(y_true, y_pred, target_names=["FALSE","TRUE"]))
print("Confusion Matrix:\n", confusion_matrix(y_true, y_pred))

# 7.9 ROC-AUC & accuracy on test
from sklearn.metrics import roc_curve, auc

model.eval()
y_true_list, y_probs = [], []
with torch.no_grad():
    for X, y in test_loader:
        X = X.to(device)
        logits = model(X)
        probs = F.softmax(logits, dim=1)[:, 1].cpu().numpy()  # prob of TRUE
        y_probs.extend(probs)
        y_true_list.extend(y.numpy())

y_true_arr = np.array(y_true_list)
y_prob_arr = np.array(y_probs)
y_pred_arr = (y_prob_arr >= 0.5).astype(int)

test_acc = accuracy_score(y_true_arr, y_pred_arr)
test_auc = roc_auc_score(y_true_arr, y_prob_arr)
print(f"TEST Accuracy: {test_acc:.4f}")
print(f"TEST ROC-AUC : {test_auc:.4f}")

# STEP 7.10 ROC Curve Plot (Bi-LSTM + attention + Stacked GRU)
import matplotlib.pyplot as plt

fpr, tpr, _ = roc_curve(y_true_arr, y_prob_arr)
roc_auc = auc(fpr, tpr)

plt.figure(figsize=(6,5))
plt.plot(fpr, tpr, label=f"BiLSTM ROC (AUC = {roc_auc:.4f})")
plt.plot([0,1], [0,1], linestyle="--", color="gray")
plt.xlabel("False Positive Rate"); plt.ylabel("True Positive Rate")
plt.title("ROC Curve ‚Äî BiLSTM (Test)")
plt.legend(loc="lower right"); plt.grid(True)
roc_path = os.path.join(SAVE_DIR, "bilstm_roc_curve.png")
plt.savefig(roc_path, bbox_inches="tight", dpi=150)
plt.show()
print("Saved ROC curve image ‚Üí", roc_path)

# 7.11 Train vs Test ROC curves
def get_roc_data(dataloader, name="Test"):
    y_true, y_scores = [], []
    with torch.no_grad():
        for X, y in tqdm(dataloader, desc=f"Generating ROC data ({name})"):
            X = X.to(device)
            logits = model(X)
            probs = F.softmax(logits, dim=1)[:, 1].cpu().numpy()
            y_true.extend(y.numpy())
            y_scores.extend(probs)
    fpr, tpr, _ = roc_curve(y_true, y_scores)
    auc_score = auc(fpr, tpr)
    return fpr, tpr, auc_score

fpr_train, tpr_train, auc_train = get_roc_data(train_loader, name="Train")
fpr_test, tpr_test, auc_test = get_roc_data(test_loader, name="Test")

plt.figure(figsize=(7,6))
plt.plot(fpr_train, tpr_train, label=f"Train (AUC = {auc_train:.4f})")
plt.plot(fpr_test, tpr_test, label=f"Test  (AUC = {auc_test:.4f})")
plt.plot([0,1],[0,1], linestyle="--", color="gray")
plt.xlabel("False Positive Rate"); plt.ylabel("True Positive Rate")
plt.title("Train vs Test ROC ‚Äî Hybrid Model")
plt.legend(loc="lower right"); plt.grid(alpha=0.25)
plt.show()

print(f"‚úÖ Train AUC: {auc_train:.4f}")
print(f"‚úÖ Test  AUC: {auc_test:.4f}")

if abs(auc_train - auc_test) <= 0.02:
    print("üéØ Model generalizes well (no overfitting).")
elif abs(auc_train - auc_test) <= 0.05:
    print("‚ö†Ô∏è Slight overfitting, but acceptable.")
else:
    print("üö® Significant overfitting detected ‚Äî check regularization or data balance.")

# 7.12 Save predictions
preds_df = pd.DataFrame({
    "y_true": y_true_arr,
    "y_prob_TRUE": y_prob_arr,
    "y_pred": y_pred_arr
})

preds_path = os.path.join(SAVE_DIR, "hybrid_test_predictions.csv")
preds_df.to_csv(preds_path, index=False)
print("‚úÖ Saved hybrid test predictions ‚Üí", preds_path)

from google.colab import drive
drive.mount('/content/drive')

# FUSION 1 ‚Äì Imports, config, paths

import os, random, numpy as np, pandas as pd, torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, classification_report, confusion_matrix
from tqdm import tqdm
import matplotlib.pyplot as plt

from transformers import AutoTokenizer, AutoModelForSequenceClassification
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Reproducibility
SEED = 42
random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Device:", device)

# Paths
BASE = "/content/drive/MyDrive/Major_Project/IFND_dataset"
TRAIN_PATH = f"{BASE}/IFND_train.csv"
VAL_PATH   = f"{BASE}/IFND_val.csv"
TEST_PATH  = f"{BASE}/IFND_test.csv"

ROBERTA_BEST_PATH = f"{BASE}/models_roberta/roberta-best.pt"
HYBRID_BEST_PATH  = f"{BASE}/models_hybrid/hybrid-best.pt"

# Labels mapping
LABEL2ID = {"FALSE":0, "TRUE":1}
ID2LABEL = {v:k for k,v in LABEL2ID.items()}

# Hyperparams
ROBERTA_NAME = "roberta-base"
MAX_LEN = 128        # same as before
BATCH_SIZE_FUSION = 32
EPOCHS_FUSION = 10
PATIENCE_FUSION = 3
LR_FUSION = 1e-3

SAVE_DIR_FUSION = f"{BASE}/models_fusion"
os.makedirs(SAVE_DIR_FUSION, exist_ok=True)

# FUSION 2 ‚Äì Load train/val/test splits & prepare Hybrid (Keras) tokenizer

train_df = pd.read_csv(TRAIN_PATH)
val_df   = pd.read_csv(VAL_PATH)
test_df  = pd.read_csv(TEST_PATH)

# map labels to IDs (same as before)
for d in [train_df, val_df, test_df]:
    d["Label"] = d["Label"].astype(str).str.upper().map(LABEL2ID)

y_train = train_df["Label"].values
y_val   = val_df["Label"].values
y_test  = test_df["Label"].values

# Keras Tokenizer for Hybrid side (same logic as your hybrid model)
keras_tokenizer = Tokenizer(oov_token="<OOV>")
keras_tokenizer.fit_on_texts(train_df["clean_text"])
vocab_size = len(keras_tokenizer.word_index) + 1
print("Vocab size (Hybrid):", vocab_size)

def encode_texts_keras(texts):
    seq = keras_tokenizer.texts_to_sequences(texts)
    return pad_sequences(seq, maxlen=MAX_LEN, padding='post', truncating='post')

X_train_hyb = encode_texts_keras(train_df["clean_text"])
X_val_hyb   = encode_texts_keras(val_df["clean_text"])
X_test_hyb  = encode_texts_keras(test_df["clean_text"])

print("Hybrid sequences shapes:", X_train_hyb.shape, X_val_hyb.shape, X_test_hyb.shape)

# FUSION 3 ‚Äì Dataset class combining RoBERTa tokens + Hybrid sequences

roberta_tokenizer = AutoTokenizer.from_pretrained(ROBERTA_NAME)

class FusionDataset(Dataset):
    def __init__(self, texts, labels, roberta_tok, hybrid_seqs, max_len):
        self.texts = texts.tolist()
        self.labels = labels.tolist()
        self.roberta_tok = roberta_tok
        self.hybrid_seqs = torch.tensor(hybrid_seqs, dtype=torch.long)
        self.max_len = max_len

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])

        # RoBERTa tokenization
        enc = self.roberta_tok(
            text,
            max_length=self.max_len,
            truncation=True,
            padding="max_length",
            return_tensors="pt"
        )
        item = {k: v.squeeze(0) for k,v in enc.items()}

        # Hybrid sequence
        item["hyb_seq"] = self.hybrid_seqs[idx]

        # Label
        item["labels"] = torch.tensor(int(self.labels[idx]), dtype=torch.long)
        return item

train_fusion_ds = FusionDataset(train_df["clean_text"], y_train, roberta_tokenizer, X_train_hyb, MAX_LEN)
val_fusion_ds   = FusionDataset(val_df["clean_text"],   y_val,   roberta_tokenizer, X_val_hyb,   MAX_LEN)
test_fusion_ds  = FusionDataset(test_df["clean_text"],  y_test,  roberta_tokenizer, X_test_hyb,  MAX_LEN)

train_fusion_loader = DataLoader(train_fusion_ds, batch_size=BATCH_SIZE_FUSION, shuffle=True)
val_fusion_loader   = DataLoader(val_fusion_ds,   batch_size=BATCH_SIZE_FUSION, shuffle=False)
test_fusion_loader  = DataLoader(test_fusion_ds,  batch_size=BATCH_SIZE_FUSION, shuffle=False)

len(train_fusion_loader), len(val_fusion_loader), len(test_fusion_loader)

# FUSION 4 ‚Äì Attention + HybridBiLSTM_Attn_GRU (same as before, plus encode() method)

class AttentionLayer(nn.Module):
    def __init__(self, hidden_dim):
        super().__init__()
        self.attn = nn.Linear(hidden_dim * 2, hidden_dim)
        self.v = nn.Linear(hidden_dim, 1, bias=False)

    def forward(self, lstm_outputs):
        # lstm_outputs: [batch, seq_len, 2*hidden]
        scores = self.v(torch.tanh(self.attn(lstm_outputs))).squeeze(-1)  # [batch, seq_len]
        weights = F.softmax(scores, dim=1)  # [batch, seq_len]
        weighted_output = (lstm_outputs * weights.unsqueeze(-1)).sum(dim=1)  # [batch, 2*hidden]
        return weighted_output, weights

class HybridBiLSTM_Attn_GRU(nn.Module):
    def __init__(self, vocab_size, embed_dim=256, hidden_dim=128,
                 num_gru_layers=1, dropout=0.3):
        super().__init__()

        self.hidden_dim = hidden_dim
        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)

        self.bilstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True,
                              bidirectional=True, dropout=dropout)

        self.attn = AttentionLayer(hidden_dim)

        self.gru = nn.GRU(hidden_dim * 2, hidden_dim,
                          num_layers=num_gru_layers,
                          batch_first=True,
                          bidirectional=False,
                          dropout=dropout)

        self.fc = nn.Sequential(
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, 128),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(128, 2)
        )

    def encode(self, x):
        # x: [batch, seq_len]
        x = self.embedding(x)
        lstm_out, _ = self.bilstm(x)
        attn_vec, _ = self.attn(lstm_out)
        gru_input = attn_vec.unsqueeze(1)
        gru_out, _ = self.gru(gru_input)
        gru_final = gru_out[:, -1, :]   # [batch, hidden_dim]
        return gru_final

    def forward(self, x):
        gru_final = self.encode(x)
        logits = self.fc(gru_final)
        return logits

# FUSION 5 ‚Äì Load fine-tuned RoBERTa and Hybrid models

# ----- RoBERTa (classification model with hidden states) -----
roberta_model = AutoModelForSequenceClassification.from_pretrained(
    ROBERTA_NAME,
    num_labels=2,
    id2label=ID2LABEL,
    label2id=LABEL2ID
).to(device)

roberta_state = torch.load(ROBERTA_BEST_PATH, map_location=device)
roberta_model.load_state_dict(roberta_state)
roberta_model.eval()

# Freeze RoBERTa parameters (feature extractor mode)
for p in roberta_model.parameters():
    p.requires_grad = False

# ----- Hybrid model -----
checkpoint_hyb = torch.load(HYBRID_BEST_PATH, map_location=device)

hybrid_model = HybridBiLSTM_Attn_GRU(
    vocab_size=checkpoint_hyb['vocab_size'],
    embed_dim=checkpoint_hyb['embed_dim'],
    hidden_dim=checkpoint_hyb['hidden_dim'],
    num_gru_layers=1,
    dropout=0.3
).to(device)

hybrid_model.load_state_dict(checkpoint_hyb['model_state_dict'])
hybrid_model.eval()

# Freeze Hybrid parameters (feature extractor mode)
for p in hybrid_model.parameters():
    p.requires_grad = False

print("RoBERTa & Hybrid loaded and frozen.")

# FUSION 6 ‚Äì Fusion model (RoBERTa CLS + Hybrid GRU hidden ‚Üí classifier)

class FusionModel(nn.Module):
    def __init__(self, roberta_model, hybrid_model, fusion_dropout=0.3):
        super().__init__()
        self.roberta = roberta_model
        self.hybrid  = hybrid_model

        fusion_dim = 768 + 128   # RoBERTa CLS (768) + Hybrid hidden (128)

        self.classifier = nn.Sequential(
            nn.Linear(fusion_dim, 256),
            nn.ReLU(),
            nn.Dropout(fusion_dropout),
            nn.Linear(256, 2)
        )

    def forward(self, input_ids, attention_mask, hyb_seq):
        # ---- RoBERTa CLS embedding ----
        with torch.no_grad():
            outputs = self.roberta(
                input_ids=input_ids,
                attention_mask=attention_mask,
                output_hidden_states=True
            )
            cls_vec = outputs.hidden_states[-1][:, 0, :]   # [B, 768]

        # ---- Hybrid GRU embedding ----
        with torch.no_grad():
            gru_vec = self.hybrid.encode(hyb_seq)          # [B, 128]

        fused = torch.cat([cls_vec, gru_vec], dim=1)       # [B, 896]
        logits = self.classifier(fused)
        return logits

fusion_model = FusionModel(roberta_model, hybrid_model).to(device)
print(fusion_model)

# FUSION 7 ‚Äì Training setup (only classifier head trainable)

# Only parameters of classifier will be updated
optimizer = torch.optim.Adam(fusion_model.classifier.parameters(), lr=LR_FUSION)
criterion = nn.CrossEntropyLoss()

best_val_f1 = -1
wait_fusion = 0
best_fusion_path = os.path.join(SAVE_DIR_FUSION, "fusion-best.pt")

# FUSION 8 ‚Äì Evaluation helper

@torch.no_grad()
def evaluate_fusion(model, loader):
    model.eval()
    all_preds, all_labels, losses, all_probs = [], [], [], []
    for batch in loader:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        hyb_seq = batch["hyb_seq"].to(device)
        labels  = batch["labels"].to(device)

        logits = model(input_ids, attention_mask, hyb_seq)
        loss = criterion(logits, labels)

        probs = F.softmax(logits, dim=1)[:, 1].cpu().numpy()
        preds = torch.argmax(logits, dim=1)

        all_probs.extend(probs)
        all_preds.extend(preds.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())
        losses.append(loss.item())

    avg_loss = np.mean(losses) if len(losses) > 0 else 0.0
    f1 = f1_score(all_labels, all_preds, average="macro")
    acc = accuracy_score(all_labels, all_preds)
    auc = roc_auc_score(all_labels, np.array(all_probs))

    return avg_loss, f1, acc, auc, np.array(all_labels), np.array(all_preds), np.array(all_probs)

# FUSION 9 ‚Äì Training loop

for epoch in range(1, EPOCHS_FUSION + 1):
    fusion_model.train()
    running = 0.0

    pbar = tqdm(train_fusion_loader, desc=f"Fusion Epoch {epoch}/{EPOCHS_FUSION}")
    for batch in pbar:
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        hyb_seq = batch["hyb_seq"].to(device)
        labels  = batch["labels"].to(device)

        optimizer.zero_grad()
        logits = fusion_model(input_ids, attention_mask, hyb_seq)
        loss = criterion(logits, labels)
        loss.backward()
        torch.nn.utils.clip_grad_norm_(fusion_model.classifier.parameters(), max_norm=1.0)
        optimizer.step()

        running += loss.item()
        pbar.set_postfix({"train_loss": f"{running/(pbar.n or 1):.4f}"})

    # Validation
    val_loss, val_f1, val_acc, val_auc, _, _, _ = evaluate_fusion(fusion_model, val_fusion_loader)
    print(f"\nEpoch {epoch}: val_loss={val_loss:.4f} | val_F1={val_f1:.4f} | val_acc={val_acc:.4f} | val_AUC={val_auc:.4f}")

    if val_f1 > best_val_f1:
        best_val_f1 = val_f1
        wait_fusion = 0
        torch.save(fusion_model.state_dict(), best_fusion_path)
        print(f"‚úÖ New best fusion model saved ‚Üí {best_fusion_path}")
    else:
        wait_fusion += 1
        if wait_fusion >= PATIENCE_FUSION:
            print("‚èπÔ∏è Early stopping (Fusion).")
            break

print("Best Val F1 (Fusion):", best_val_f1)

# FUSION 10 ‚Äì Load best fusion model & evaluate on TEST

fusion_model.load_state_dict(torch.load(best_fusion_path, map_location=device))
fusion_model.to(device)

test_loss, test_f1, test_acc, test_auc, y_true_f, y_pred_f, y_prob_f = evaluate_fusion(fusion_model, test_fusion_loader)

print(f"FUSION TEST ‚Äî loss: {test_loss:.4f} | macro-F1: {test_f1:.4f}")
print(f"FUSION TEST ‚Äî Accuracy: {test_acc:.4f} | ROC-AUC: {test_auc:.4f}")

print("\nClassification Report (Fusion):\n", classification_report(y_true_f, y_pred_f, target_names=["FALSE","TRUE"]))
print("Confusion Matrix (Fusion):\n", confusion_matrix(y_true_f, y_pred_f))

# FUSION 11 ‚Äì ROC Curve for Fusion model

from sklearn.metrics import roc_curve, auc

fpr_f, tpr_f, _ = roc_curve(y_true_f, y_prob_f)
roc_auc_f = auc(fpr_f, tpr_f)

plt.figure(figsize=(6,5))
plt.plot(fpr_f, tpr_f, label=f"Fusion ROC (AUC = {roc_auc_f:.4f})")
plt.plot([0,1], [0,1], linestyle="--", color="gray")
plt.xlabel("False Positive Rate"); plt.ylabel("True Positive Rate")
plt.title("ROC Curve ‚Äî Fusion Model (Test)")
plt.legend(loc="lower right"); plt.grid(True)
fusion_roc_path = os.path.join(SAVE_DIR_FUSION, "fusion_roc_curve.png")
plt.savefig(fusion_roc_path, bbox_inches="tight", dpi=150)
plt.show()
print("Saved Fusion ROC curve image ‚Üí", fusion_roc_path)

# FUSION 12 ‚Äì Train vs Test ROC curves
from sklearn.metrics import roc_curve, auc

@torch.no_grad()
def get_roc_data_fusion(dataloader, name="Test"):
    y_true, y_scores = [], []
    for batch in tqdm(dataloader, desc=f"Generating ROC data ({name})"):
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        hyb_seq = batch["hyb_seq"].to(device)
        labels = batch["labels"].cpu().numpy()

        logits = fusion_model(input_ids, attention_mask, hyb_seq)
        probs = F.softmax(logits, dim=1)[:, 1].cpu().numpy()

        y_true.extend(labels)
        y_scores.extend(probs)

    fpr, tpr, _ = roc_curve(y_true, y_scores)
    auc_score = auc(fpr, tpr)
    return fpr, tpr, auc_score

# ‚ö†Ô∏è Ensure best model loaded:
fusion_model.load_state_dict(torch.load(best_fusion_path, map_location=device))
fusion_model.to(device)
fusion_model.eval()

fpr_train_f, tpr_train_f, auc_train_f = get_roc_data_fusion(train_fusion_loader, name="Train")
fpr_test_f,  tpr_test_f,  auc_test_f  = get_roc_data_fusion(test_fusion_loader,  name="Test")

plt.figure(figsize=(7,6))
plt.plot(fpr_train_f, tpr_train_f, label=f"Train (AUC = {auc_train_f:.4f})")
plt.plot(fpr_test_f,  tpr_test_f,  label=f"Test  (AUC = {auc_test_f:.4f})")
plt.plot([0,1], [0,1], linestyle="--", color="gray")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("Train vs Test ROC ‚Äî Fusion Model")
plt.legend(loc="lower right")
plt.grid(alpha=0.25)
plt.show()

print(f"‚úÖ Train AUC: {auc_train_f:.4f}")
print(f"‚úÖ Test  AUC: {auc_test_f:.4f}")

if abs(auc_train_f - auc_test_f) <= 0.02:
    print("üéØ Model generalizes well (no overfitting).")
elif abs(auc_train_f - auc_test_f) <= 0.05:
    print("‚ö†Ô∏è Slight overfitting, but acceptable.")
else:
    print("üö® Significant overfitting detected ‚Äî check regularization or data balance.")

# FUSION 13 ‚Äì Save per-sample predictions

fusion_preds_df = pd.DataFrame({
    "y_true": y_true_f,
    "y_prob_TRUE": y_prob_f,
    "y_pred": y_pred_f
})

fusion_preds_path = os.path.join(SAVE_DIR_FUSION, "fusion_test_predictions.csv")
fusion_preds_df.to_csv(fusion_preds_path, index=False)
print("‚úÖ Saved fusion test predictions ‚Üí", fusion_preds_path)

import torch
import tensorflow as tf
import transformers
import sklearn
import numpy as np
import pandas as pd

print(f"PyTorch: {torch.__version__}")
print(f"TensorFlow: {tf.__version__}")
print(f"Transformers: {transformers.__version__}")
print(f"Scikit-learn: {sklearn.__version__}")
print(f"NumPy: {np.__version__}")
print(f"Pandas: {pd.__version__}")